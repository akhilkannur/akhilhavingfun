<h1
id="the-hybrid-prompt-a-simple-fix-for-agents-that-get-stuck-or-lose-context">The
“Hybrid” prompt: A simple fix for agents that get stuck or lose
context</h1>
<h2 id="tips-and-tricks">Tips and Tricks</h2>
<p>I’ve been testing a lot of automations lately with tools like Gemini
CLI. I have a huge list of about 500 prompts that worked fine when I was
chatting with them, but they all fell apart once I tried to let them run
on their own in the background. So I thought about sharing a few
observations I noticed about agents and the fix I came up with. This
could be a “too long, didn’t read” one, but I’ll give it a go!</p>
<p>The issue is that most prompts are built for a back and forth
conversation. They expect you to be there to fix things or give them a
nudge. If you want a tool to just do the work without you watching it,
that conversational style is actually the problem.</p>
<p>I had to stop thinking about prompts as instructions and start
thinking of them as Processors. I now use three basic categories for
every task.</p>
<h2 id="the-3-categories-for-reliable-work">The 3 Categories for
Reliable Work</h2>
<ul>
<li><p><strong>The Processor:</strong> This is for moving data. You give
it a file, it extracts what matters, and it stops. No searching or
talking. Just data in and data out.</p></li>
<li><p><strong>The Researcher:</strong> This starts from zero. You give
it a goal and it goes out to find the facts to build a
foundation.</p></li>
<li><p><strong>The Hybrid:</strong> This is the most reliable for long
tasks. It checks if you already gave it a list of companies or URLs. If
that file is missing, it finds the info itself, then starts the actual
work.</p></li>
</ul>
<h2 id="example-the-product-roadmap-inference-engine-hybrid">Example:
The Product Roadmap Inference Engine (Hybrid)</h2>
<p>Instead of just asking for a summary, this is meant to connect dots
between different sources to guess what a company is building next.</p>
<p><strong>Role:</strong> Strategic Market Intelligence Agent.</p>
<p><strong>Objective:</strong> Guess a competitor’s likely product
roadmap by connecting signals from their job boards and recent technical
documentation.</p>
<h3 id="phase-1-the-initial-check">Phase 1: The Initial Check</h3>
<p>Look for a file called target_competitors.csv. If it is missing,
search for the top competitors in this niche and save them to the file.
If it is already there, just load it</p>
<h3 id="phase-2-the-collection-loop">Phase 2: The Collection Loop</h3>
<p>For each company on the list:</p>
<ul>
<li><p>Check their careers page for technical hires. If they are hiring
for a specific skill but do not have that feature yet, that is a
signal.</p></li>
<li><p>Check their changelog or “What’s New” page for recent
releases.</p></li>
<li><p>Look at their public documentation for any new versions or beta
features.</p></li>
</ul>
<h3 id="phase-3-the-guess">Phase 3: The Guess</h3>
<p>Compare the hiring trends against the current product. If they are
hiring for a specific tech but the product lacks it, they are likely
launching it soon. Save the final guess to a report file. No chat
commentary is allowed during the process to keep the results clean.</p>
<h2 id="tldr">TL;DR</h2>
<p>The problem I found is that the “social” side of AI is actually a
liability for automation. In a normal chat, the model is trained to be
helpful, so it wants to give you updates, ask for feedback, or summarize
its progress between every step.</p>
<p>When you are trying to process hundreds of rows of data, those small
“social” interruptions actually break the logic. All that extra
conversational text eventually fills up the model’s memory (the context
window) until it forgets the original objective. Stripping out the chat
and forcing it into a strict phase structure (Check, Loop, Save) is the
best way I can get a script to stay on track without me baby-sitting
it.</p>
